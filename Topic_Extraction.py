# -*- coding: utf-8 -*-
"""Data Mining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WIDhjHemJ8VCzd2C7iQjcNo8w_yi3ayz

**Data Mining - Topic Extraction from StackOverflow Data in the Context of Software Architecture.**

# Import statements
"""

!pip install stackapi
!pip install s3fs
!pip install pandas
!pip install numpy
!pip install nltk
!pip install sklearn
!pip install keras
!pip install regex
!pip install matplotlib
!pip install bs4 lxml

!pip install lda
!pip install tensorflow

import pandas as pd     
import numpy as np
import s3fs
import matplotlib.pyplot as plt

import nltk
from nltk.corpus import stopwords 
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

import regex as re

import matplotlib.pyplot as plt

from keras.preprocessing.text import Tokenizer 

from stackapi import StackAPI

import seaborn as sns
sns.set(style="whitegrid")

from scipy import stats

import lda

from bokeh.palettes import Set3, Blues8, brewer

from sklearn.feature_extraction.text import CountVectorizer

import matplotlib.pyplot as plt

pd.options.mode.chained_assignment = None

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

"""# Data Extraction using StackAPI"""

#key is required for extracting more than 500 posts from the stackoverflow api.
#maximum posts retrieved using this query will be max_pages * page_size = 100,000
SITE = StackAPI('stackoverflow', max_pages=1000, page_size=100, key='kGCevKwTYZ)K3MXyECOpmg((')

#basically we are collecting ten years worth of data
#1262304000 date refers to 01-01-2010
questions = SITE.fetch('posts', tagged='software-design', filter='withbody', fromdate=1262304000)

#store the indexed content of the posts along with the score
import csv
stackoverflow_data = []
i =  1
for item in questions['items']:
  stackoverflow_data.append({'id': i, 'contents': item['body'], 'score':item['score']})
  i = i + 1
 
csv_file = "stackoverflow_data.csv"
with open(csv_file, 'w') as csvfile:
  writer = csv.DictWriter(csvfile, fieldnames=['id','contents','score'])
  writer.writeheader()
  for data in stackoverflow_data:
    writer.writerow(data)

#Verify that stackoverflow data is accessible
# !pip install google-colab
import os
from google.colab import drive
drive.mount('/content/drive/')
!ls "/content/drive/My Drive/stackoverflow_data"

# READ DATASET INTO DATAFRAME

df = pd.read_csv("/content/drive/My Drive/stackoverflow_data.csv")

from google.colab import drive
drive.mount('/content/drive')

# Sampling the dataset
df  = df.sample(n=25000, random_state=1)

df.head

"""# Dataset view"""

# SHAPE of DATAFRAME

df.shape

# VIEW OF A DATAFRAME

df.head(5)

# VIEW OF A RAW contents

df.iloc[0, 1]

"""## Upvote-Score Analysis"""

(df["score"].describe())

"""# Data Preprocessing

### Remove code from Contents column
"""

#Remove all the code details from the posts
from bs4 import BeautifulSoup as bs
import lxml

new_data = []
for post in df['contents']:
  data = bs(post, 'lxml')
  for tag in data.find_all('code'):
    tag.decompose()
  new_data.append(data)
df['contents']  = [ str(item) for item in new_data]

# DROP ROWS WHICH HAS SAME contents
df.drop_duplicates(subset=['contents'],inplace=True)  #dropping duplicates
# DROP ROWS WITH NA VALUES
df.dropna(axis=0,inplace=True)

# NEW/CURRENT SHAPE of DATAFRAME

df.shape

# ALTHOUGH IT SEEMS THAT WE DON'T HAVE ANY CONTRACTIONS BUT IT IS A GOOD PRACTICE TO CHECK IF THEY EXIST AND REMOVE IF THEY ARE THERE.

df[df['contents'].str.match('\'')]

"""### Remove URLs"""

# Identify records with URL and Drop records with URL in summary
# https://www.geeksforgeeks.org/python-check-url-string/

def remove_urls(df):
  """
  This method removes the records containing URLs in the contents section
  """
    
  url_regex = r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?«»“”‘’]))"

  print("Total records:", len(df))

  df['hasURL'] = df['contents'].apply(lambda contents : bool(len([x[0] for x in re.findall(url_regex, contents)])))
  df = df[df['hasURL']==False]
  df.drop(columns=['hasURL'], inplace=True)           # dropping the 'hasURL' column
  return df

df = remove_urls(df)

df.shape

"""### Lower-case conversion, Remove HTML tags and stop-words"""

stop_words = set(stopwords.words('english'))              # Prepare a set of STOPWORDS

def summary_cleaner(text):

  newString = text.lower()                                # CONVERT INTO LOWER CASE
  newString = re.sub(re.compile('<.*?>'), " ", newString) # REMOVE HTML TAGS
  newString = re.sub(r'\([^)]*\)', '', newString)         # REMOVE SMALL BRACKETS i.e "(xxxx)" => xxxx
  newString = re.sub('"','', newString)                   # REMOVE INVERTED COMMA'S
  newString = re.sub(r"'s\b","",newString)                # REMOVE 's FROM THE LAST OF ANY TOKEN
  newString = re.sub("[^a-zA-Z]", " ", newString)         # REMOVE NUMERIC content.

  tokens = [w for w in newString.split() if not w in stop_words]    # REMOVE STOP WORDS
  return (" ".join(tokens)).strip()

cleaned_summary = []
for text in df['contents']:
  cleaned_summary.append(summary_cleaner(text))

# VIEW OF A CLEAN SUMMARY

cleaned_summary[0]

len(cleaned_summary)

# VIEW OF RAW Contents

df.iloc[0, 1]

# # ADD the CLEANED SUMMARY AND HEADLINE INTO DATAFRAME IN NEW COLUMNS

df['cleaned_contents']= cleaned_summary

"""### Cleaned Dataset view"""

pd.set_option('display.width', 1000)
df.iloc[0:5, 3]

writer = pd.ExcelWriter('Cleaned_Data.xlsx')
df.to_excel(writer)
writer.save()

# populate the lists with sentence lengths
summary_word_count = [len(i.split()) for i in df['contents']]

length_df = pd.DataFrame({'Contents':summary_word_count})
length_df.hist(bins=80, figsize=(10,5) )
plt.suptitle("Distribution of Contents", size=8)
plt.show()

"""#### Effective Lemmatization - Tokenization, POS Tagging, POS Tagging - Wordnet, Lemmatization"""

# Word Tokenization of Sequences
def tokenize_dataset(data):
  """
  This method is used to convert input data sequence into tokenized word sequences.
  """
  return([nltk.word_tokenize(samples) for samples in data])

# POS Tagging of Tokens
def pos_tagging_tokens(data):
  return([nltk.pos_tag(samples) for samples in data])

# X Values
# Calling Tokenization method on Data sequences
tokens_X_train = tokenize_dataset(df["cleaned_contents"])

# Calling POS tagging method on Tokenized sequences
tagged_tokens_X_train = pos_tagging_tokens(tokens_X_train)

print(tokens_X_train[10])

print(tagged_tokens_X_train[1])

"""#### Converting POS Tags to WordNet Tags"""

wnl = WordNetLemmatizer()

def pos_tag_wordnet(data):
  """
    This method converts POS tags of input sequences to Effective Lemmatizaton tags sequences.
  """
  new_tagged_tokens = []
  tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}
  for tagged_tokens in data:
    new_tagged_tokens.append([(word, tag_map.get(tag[0].lower(), wordnet.NOUN)) for word, tag in tagged_tokens])
  return new_tagged_tokens

# X Values
new_tagged_tokens_X_train = pos_tag_wordnet(tagged_tokens_X_train)

len(new_tagged_tokens_X_train)

# Create lemmatization 
def lemmatized_text(wordnet_tokens_data):
  """
  This method converts input tokenized sequence into lemmatized text sequence.
  """
  return([ ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens) for wordnet_tokens in wordnet_tokens_data])

# X Values
lemmatized_text_X_train = lemmatized_text(new_tagged_tokens_X_train)

print(lemmatized_text_X_train[0])

X_tokenizer = Tokenizer()
X_tokenizer.fit_on_texts(list(lemmatized_text_X_train))

# Summary vocab size

X_vocabulary_size   =  len(X_tokenizer.word_index) +1

print("Size of Vocabulary for Cleaned Contents:", X_vocabulary_size)

"""# Performing LDA - Topic Modelling

## Converting data into Term-Frequency Matrix (tf, unigrams)
"""

cv = CountVectorizer(min_df=0., max_df=1.)
cv_matrix = cv.fit_transform(lemmatized_text_X_train)
cv_matrix = cv_matrix.toarray()
cv_matrix

# get all unique words in the corpus
vocab = cv.get_feature_names()
# show document feature vectors
pd.DataFrame(cv_matrix, columns=vocab)

"""## Train the Model"""

model = lda.LDA(n_topics=30, n_iter=20000, random_state=100)
# cause and effect relationship, Conclusion Validity , End output changes by changing these params

X = cv_matrix

model.fit(X)

topic_word = None
topic_word = model.topic_word_

"""# Results

##Extracting "K" topics
"""

import joblib

joblib.dump(model, '/content/drive/My Drive/lda_model.jl')

# Reload the model object
import joblib
reload_model = joblib.load('/content/drive/My Drive/lda_model.jl')
doc_topic = None
topic_word= None
doc_topic = reload_model.doc_topic_
topic_word = reload_model.topic_word_

n_top_words = 100
for i, topic_dist in enumerate(topic_word):
  topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
  print('Topic {}: {}'.format(i, ' '.join(topic_words)))

"""## Document-topic distributions/ Percentage share"""

for i in range(20):
  print("Stackoverflow Question index: {} (top topic: {})".format(df.iloc[i,0], doc_topic[i].argmax()))

df

df["topic_number"] = list(map(lambda x: x.argmax(), doc_topic))

data = df["topic_number"].value_counts().sort_index()
data

plt.figure(figsize = (15, 8))
plt.bar(np.arange(data.shape[0]), data, color = 'orange')
plt.xticks(np.arange(data.shape[0]))
plt.grid(False)
plt.xlabel("Topic")
plt.ylabel("Frequency")
plt.savefig("topic.pdf")

data_np = data.to_numpy()
sorted_data = data.sort_values(ascending=False)
sorted_data.values

sorted_data.plot.pie(y='mass', figsize=(10, 10), autopct= '%1.2f%%', pctdistance=1.15, 
                     labeldistance=0.7, radius= 0.8, shadow=True,  fontsize=11, 
                     colors = brewer['Dark2'][8] +brewer["Paired"][10]+ Blues8[2:7]+brewer["Accent"][8])
plt.ylabel("Topic Numbers with Percentage share")
plt.savefig("pie_chart.png")